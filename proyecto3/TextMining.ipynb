{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python39\\python.exe: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] El sistema no puede encontrar la ruta especificada: 'reviews'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\DELL\\Data Mining\\Data_Mining\\proyecto3\\TextMining.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DELL/Data%20Mining/Data_Mining/proyecto3/TextMining.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m##Escoge 100 archivos al azar\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DELL/Data%20Mining/Data_Mining/proyecto3/TextMining.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m folder_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mreviews\u001b[39m\u001b[39m\"\u001b[39m  \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/DELL/Data%20Mining/Data_Mining/proyecto3/TextMining.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m all_files \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(folder_path)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DELL/Data%20Mining/Data_Mining/proyecto3/TextMining.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m random_files \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39msample(all_files, \u001b[39m100\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/DELL/Data%20Mining/Data_Mining/proyecto3/TextMining.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(random_files)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] El sistema no puede encontrar la ruta especificada: 'reviews'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "##Escoge 100 archivos al azar\n",
    "folder_path = \"reviews\"  \n",
    "all_files = os.listdir(folder_path)\n",
    "random_files = random.sample(all_files, 100)\n",
    "print(random_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': ['10820_1.txt', '3765_1.txt', '5885_1.txt', '6881_1.txt', '4704_1.txt', '4852_1.txt', '9325_1.txt', '332_1.txt', '11718_1.txt', '5945_1.txt', '3533_1.txt', '6992_1.txt', '2353_1.txt', '2094_1.txt', '12297_1.txt', '2769_1.txt', '1692_1.txt', '3731_1.txt', '6239_1.txt', '5903_1.txt', '1506_1.txt', '4478_1.txt', '3314_1.txt', '4075_1.txt', '7039_1.txt', '3207_1.txt', '4796_1.txt', '308_1.txt', '8013_1.txt', '9944_1.txt', '6641_1.txt', '5864_1.txt', '10784_1.txt'], '4': ['6183_4.txt', '6336_4.txt', '287_4.txt', '6727_4.txt', '11765_4.txt', '12023_4.txt', '1637_4.txt', '5391_4.txt', '3823_4.txt', '2255_4.txt', '8888_4.txt', '10258_4.txt', '4012_4.txt', '2970_4.txt', '649_4.txt', '8194_4.txt', '5694_4.txt', '9545_4.txt', '390_4.txt', '7408_4.txt', '10436_4.txt', '8568_4.txt', '4789_4.txt', '11749_4.txt', '8044_4.txt', '5393_4.txt', '1832_4.txt', '5600_4.txt', '9598_4.txt', '9731_4.txt', '1856_4.txt', '2818_4.txt'], '2': ['11047_2.txt', '4059_2.txt', '2661_2.txt', '8932_2.txt', '11656_2.txt', '9053_2.txt', '5742_2.txt', '4340_2.txt', '11247_2.txt', '5990_2.txt', '225_2.txt', '6555_2.txt', '9617_2.txt', '8206_2.txt', '9065_2.txt', '10471_2.txt', '5611_2.txt', '2847_2.txt', '10714_2.txt', '7705_2.txt', '11123_2.txt'], '3': ['197_3.txt', '3677_3.txt', '11513_3.txt', '12456_3.txt', '12133_3.txt', '4869_3.txt', '3399_3.txt', '10457_3.txt', '3920_3.txt', '592_3.txt', '10055_3.txt', '9622_3.txt', '12325_3.txt', '26_3.txt']}\n"
     ]
    }
   ],
   "source": [
    "##Creacion de diccionario de peliculas\n",
    "movies_dict = {}\n",
    "for file in random_files:\n",
    "    new_file = file.split('_')\n",
    "    new_new_file = new_file[1].split('.')\n",
    "    key = new_new_file[0]\n",
    "    if key in movies_dict:\n",
    "        movies_dict[key].append(file)\n",
    "    else:\n",
    "        movies_dict[key] = [file]\n",
    "print(movies_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\charl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\charl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\charl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\charl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\charl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\charl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized files have been created in the 'lemmatized_reviews' directory.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "import string\n",
    "\n",
    "# Initialize NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Create a lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define the directory containing your movie files\n",
    "directory = 'reviews/'\n",
    "\n",
    "# Iterate over movies\n",
    "for movie in movies_dict:\n",
    "    moviechosen = movies_dict[movie]\n",
    "    \n",
    "    # Create a directory for lemmatized files\n",
    "    lemmatized_directory = 'lemmatized_reviews/'\n",
    "    os.makedirs(lemmatized_directory, exist_ok=True)\n",
    "    \n",
    "    for file in moviechosen:\n",
    "        path = os.path.join(directory, file)\n",
    "        \n",
    "        with open(path, 'r') as archivo:\n",
    "            text = archivo.read()\n",
    "            text=text.lower()\n",
    "            text=text.replace(\"< br / >\",\"\")\n",
    "            text=text.replace(\"<br />\",\"\")\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "            lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "            lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "            \n",
    "            # Create a new file with lemmatized content\n",
    "            lemmatized_path = os.path.join(lemmatized_directory, file)\n",
    "            with open(lemmatized_path, \"w\") as output_file:\n",
    "                output_file.write(lemmatized_text)\n",
    "\n",
    "print(\"Lemmatized files have been created in the 'lemmatized_reviews' directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "input_folder = 'lemmatized_reviews/'\n",
    "output_folder = 'reviews_by_movie/'\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "file_contents = {}\n",
    "for filename in os.listdir(input_folder):\n",
    "    file_number = filename.split(\"_\")[1].split('.')[0]\n",
    "\n",
    "    with open(os.path.join(input_folder, filename), 'r') as input_file:\n",
    "        file_content = input_file.read()\n",
    "\n",
    "    if file_number not in file_contents:\n",
    "        file_contents[file_number] = []\n",
    "    file_contents[file_number].append(file_content)\n",
    "\n",
    "for file_number, contents in file_contents.items():\n",
    "    output_file_path = os.path.join(output_folder, f\"{file_number}.txt\")\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        output_file.write(\"\\n\".join(contents))\n",
    "\n",
    "print(\"Files have been grouped and written to separate output files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "def get_term_frequency_in_file(source_file_path):\n",
    "    wordcount = {}\n",
    "    with open(source_file_path) as f:\n",
    "        translator = str.maketrans('', '', string.punctuation)  # Create a translation table\n",
    "        for line in f:\n",
    "            line = line.lower().translate(translator)  # Use the translation table to remove punctuation\n",
    "            this_wordcount = Counter(line.split())\n",
    "            wordcount = add_merge_two_dict(wordcount, this_wordcount)\n",
    "    return wordcount\n",
    "\n",
    "def add_merge_two_dict(x, y):\n",
    "    return { k: x.get(k, 0) + y.get(k, 0) for k in set(x) | set(y) }\n",
    "\n",
    "directory = 'reviews_by_movie/'\n",
    "\n",
    "for movie in movies_dict:\n",
    "    get_term_frequency_in_file(movie)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
