{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 4.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.29.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.3.0)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.25.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\charl\\miniconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (65.6.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.6.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\charl\\miniconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\charl\\miniconda3\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7575_4.txt', '10217_1.txt', '6711_2.txt', '3077_1.txt', '3211_1.txt', '845_1.txt', '11355_2.txt', '6256_3.txt', '3049_2.txt', '5625_4.txt', '6792_3.txt', '1745_2.txt', '6371_4.txt', '10192_2.txt', '4904_3.txt', '10944_1.txt', '735_1.txt', '5985_3.txt', '2743_1.txt', '11590_3.txt', '1658_1.txt', '337_4.txt', '8645_1.txt', '12037_1.txt', '2028_3.txt', '2511_4.txt', '819_1.txt', '3350_3.txt', '6938_4.txt', '3404_2.txt', '4888_1.txt', '5060_4.txt', '2254_1.txt', '3179_2.txt', '189_1.txt', '5569_4.txt', '2686_1.txt', '10197_1.txt', '5498_3.txt', '6405_3.txt', '675_3.txt', '11251_2.txt', '11955_4.txt', '8008_1.txt', '12369_1.txt', '12466_1.txt', '11201_1.txt', '7152_1.txt', '10010_3.txt', '3699_3.txt', '10074_2.txt', '11592_1.txt', '9810_2.txt', '6640_1.txt', '5320_2.txt', '4740_2.txt', '3406_4.txt', '6355_1.txt', '3686_2.txt', '4265_2.txt', '4837_1.txt', '7711_3.txt', '11696_4.txt', '2478_1.txt', '8189_4.txt', '262_4.txt', '9338_3.txt', '11141_1.txt', '6847_3.txt', '9668_1.txt', '7753_1.txt', '9639_4.txt', '1855_3.txt', '11873_1.txt', '6698_2.txt', '6971_1.txt', '637_3.txt', '1874_1.txt', '3230_4.txt', '5235_1.txt', '2926_1.txt', '4263_4.txt', '4593_1.txt', '5352_4.txt', '2249_2.txt', '4830_2.txt', '10394_3.txt', '10404_4.txt', '11611_3.txt', '11399_2.txt', '250_3.txt', '10876_1.txt', '2775_1.txt', '4988_2.txt', '8660_1.txt', '1910_1.txt', '9212_1.txt', '3448_1.txt', '2152_1.txt', '2086_3.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "##Escoge 100 archivos al azar\n",
    "folder_path = \"reviews\"  \n",
    "all_files = os.listdir(folder_path)\n",
    "random_files = random.sample(all_files, 100)\n",
    "print(random_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'4': ['7575_4.txt', '5625_4.txt', '6371_4.txt', '337_4.txt', '2511_4.txt', '6938_4.txt', '5060_4.txt', '5569_4.txt', '11955_4.txt', '3406_4.txt', '11696_4.txt', '8189_4.txt', '262_4.txt', '9639_4.txt', '3230_4.txt', '4263_4.txt', '5352_4.txt', '10404_4.txt'], '1': ['10217_1.txt', '3077_1.txt', '3211_1.txt', '845_1.txt', '10944_1.txt', '735_1.txt', '2743_1.txt', '1658_1.txt', '8645_1.txt', '12037_1.txt', '819_1.txt', '4888_1.txt', '2254_1.txt', '189_1.txt', '2686_1.txt', '10197_1.txt', '8008_1.txt', '12369_1.txt', '12466_1.txt', '11201_1.txt', '7152_1.txt', '11592_1.txt', '6640_1.txt', '6355_1.txt', '4837_1.txt', '2478_1.txt', '11141_1.txt', '9668_1.txt', '7753_1.txt', '11873_1.txt', '6971_1.txt', '1874_1.txt', '5235_1.txt', '2926_1.txt', '4593_1.txt', '10876_1.txt', '2775_1.txt', '8660_1.txt', '1910_1.txt', '9212_1.txt', '3448_1.txt', '2152_1.txt'], '2': ['6711_2.txt', '11355_2.txt', '3049_2.txt', '1745_2.txt', '10192_2.txt', '3404_2.txt', '3179_2.txt', '11251_2.txt', '10074_2.txt', '9810_2.txt', '5320_2.txt', '4740_2.txt', '3686_2.txt', '4265_2.txt', '6698_2.txt', '2249_2.txt', '4830_2.txt', '11399_2.txt', '4988_2.txt'], '3': ['6256_3.txt', '6792_3.txt', '4904_3.txt', '5985_3.txt', '11590_3.txt', '2028_3.txt', '3350_3.txt', '5498_3.txt', '6405_3.txt', '675_3.txt', '10010_3.txt', '3699_3.txt', '7711_3.txt', '9338_3.txt', '6847_3.txt', '1855_3.txt', '637_3.txt', '10394_3.txt', '11611_3.txt', '250_3.txt', '2086_3.txt']}\n"
     ]
    }
   ],
   "source": [
    "##Creacion de diccionario de peliculas\n",
    "movies_dict = {}\n",
    "for file in random_files:\n",
    "    new_file = file.split('_')\n",
    "    new_new_file = new_file[1].split('.')\n",
    "    key = new_new_file[0]\n",
    "    if key in movies_dict:\n",
    "        movies_dict[key].append(file)\n",
    "    else:\n",
    "        movies_dict[key] = [file]\n",
    "print(movies_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\charl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\charl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\charl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\charl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\charl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\charl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized files have been created in the 'lemmatized_reviews' directory.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "import string\n",
    "\n",
    "# Initialize NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Create a lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define the directory containing your movie files\n",
    "directory = 'reviews/'\n",
    "\n",
    "# Iterate over movies\n",
    "for movie in movies_dict:\n",
    "    moviechosen = movies_dict[movie]\n",
    "    \n",
    "    # Create a directory for lemmatized files\n",
    "    lemmatized_directory = 'lemmatized_reviews/'\n",
    "    os.makedirs(lemmatized_directory, exist_ok=True)\n",
    "    \n",
    "    for file in moviechosen:\n",
    "        path = os.path.join(directory, file)\n",
    "        \n",
    "        with open(path, 'r') as archivo:\n",
    "            text = archivo.read()\n",
    "            text=text.lower()\n",
    "            text=text.replace(\"< br / >\",\"\")\n",
    "            text=text.replace(\"<br />\",\"\")\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "            lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "            lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "            \n",
    "            # Create a new file with lemmatized content\n",
    "            lemmatized_path = os.path.join(lemmatized_directory, file)\n",
    "            with open(lemmatized_path, \"w\") as output_file:\n",
    "                output_file.write(lemmatized_text)\n",
    "\n",
    "print(\"Lemmatized files have been created in the 'lemmatized_reviews' directory.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
